<archimate:Contract
    xmlns:archimate="http://www.archimatetool.com/archimate"
    name="Data Quality Policy"
    id="id-168d1dbd-2a1b-4738-a221-da4bf92461db"
    documentation="Data Quality Policy&#xA;Statement&#xA;Data quality scoring provides trust and confidence in data, and a scientific basis for issue resolution and improvement.&#xA;Data quality is synonymous with information quality, since poor data quality results in inaccurate information which has a material impact on business performance. A Data Quality Assessment capability will enable Anglian Water to analyse data in order to measure and quantify its conformity to data quality expectations, i.e. score the extent to which data is ‘fit for purpose’.&#xA;Data Quality Assessment is an ongoing, incremental capability. Subject areas should be incorporated in turn, with the most cost effective areas given highest priority, after due consideration of cost v benefit v risk.&#xA;In collaboration with subject matter experts, data quality expectations are formalised as programmatic rules concerning accuracy, completeness, consistency, integrity, timeliness, uniqueness, and validity. These business rules are based on parameters such as value presence and record completeness, value domain conformance, mapping conformance, range conformance, format compliance, and conformity of data definitions. By using defined data rules proactively to validate data, Anglian Water can distinguish those records that conform to defined data quality expectations and those that do not. In turn, these data rules are used to baseline the current level of data quality as compared to ongoing assessments.&#xA;These rules form the basis of measurements and acceptability thresholds, developed with accountability, control, relevance, and stewardship in mind. Measurement will enable Anglian Water to report on the relationship between flawed data and missed business objectives. Measurements that do not meet the specified acceptability thresholds indicate non-conformance, and that some data remediation is necessary.&#xA;When extended into Business As Usual activities, a data quality incident tracking capability enables the categorisation of data issues into incident hierarchies such that they can be classified, logged, and tracked according to data quality issue resolution service level agreements. Issue resolution activity can then be standardised, with associated data governance around assignment processes, escalation procedures, and resolution workflows. The creation of a closed feedback loop will ensure that issue resolution is not only about cleansing data downstream, but also addressing the root causes of data defects at source.&#xA;Build a well-managed information source that is trusted by all &#xA;&#xA;Principles&#xA;Higher quality data means we get more decisions right. When we get them wrong it helps us to explain why.&#xA;Develop our technology and organisational capability to unlock the value within our information &#xA;We need to spend more time centrally storing information so that it can be consumed by everyone. Derived information from one team can be reused by all teams.&#xA;Digital data captured about the work we do and the quality of information we process can be used to measure and reward performance consistently, transparently and non-subjectively.&#xA;Investing sufficient time and funding in data development projects  to incorporate a discovery phase at an early stage allowing people to communicate and work across organisational boundaries, so that overlaps with data and systems elsewhere in the business are investigated, identified, and any potential synergies realised.&#xA;Simply measuring the quantity and quality of our data assets gives us the ability to track the impact of transformation against those metrics. We can see what is working and driving the right outcomes, and do more of the same. We can see where investment is not delivering a return and do something else.&#xA;Quality data that is captured, stored centrally, published and then used by many to drive value, encourages the right behaviours that benefit the whole community.&#xA;We will leverage high quality inter-connected data to automate ODI reporting, and provide operational managers with insight from lead measures, thereby giving us the best opportunity to outperform our targets and reduce the risk of penalties.&#xA;Support a more effective organisation by enabling better decision-making powered by high quality, user-friendly and joined up information  &#xA;Gives us a chance to create a next generation of ‘shared leaders’ with timely access to the information they need to perform group coaching,and delegate decision-making responsibility and accountability to their teams.&#xA;Reduce high workforce costs incurred finding, manipulating and correcting data needed for analysis, planning and decision-making&#xA;Reduce risk of representational damage and financial penalties by meeting data transparency obligations mandated by the Open Water operator&#xA;Reduce the risk of not being able to realise all of the targeted dark and light blue benefits planned from strategic programmes that are reliant on modern data management capabilities and improved data governance practices&#xA;Reduce the volume of Access databases from 500,000 to less than1,000.&#xA;Increase the opportunity to fully leverage machine learning that could be used to further increase the accuracy of predictive models&#xA;Helps us to be more proactive and avoid more expensive reactive work.&#xA;When the need for shared business-critical data exists or arises, data governance procedures and responsibilities are in place to ensure that it is properly created, integrated, maintained, used, and shared throughout the business, from an overall enterprise perspective, in line with business strategy.&#xA;High quality information fosters confidence in our decisions, builds trust and enables us to be open and transparent.&#xA;Gives us the information we need to optimise our supply chain, enabling us to buy the most competitive product and services, with minimal stock levels, and ensuring assets and people are where they need to be at the right time.&#xA;High quality data increases the opportunity to drive automation. Centralised views of the same data enables the impact of change to be seen immediately by those who need it.&#xA;Every opportunity is taken to promote the principle that the virtual asset is as important as the physical asset, and that the consequences of a data quality issue may be just as significant as a physical issue on the ground. &#xA;Every opportunity is taken to educate people about the business value of data accuracy, and the role they have to play in contributing to the realisation of that business value by ensuring accuracy of data through their actions and behaviours.&#xA;Objectives&#xA;Datasets measured for conformance with data quality expectations.&#xA;Continuously improve data quality to the level the business requires.&#xA;Manage data consistently across the enterprise.&#xA;Correct data incompleteness, inaccuracy, or outdatedness at root-cause.&#xA;Address behavioural failing in data capture.&#xA;Data quality expectations are formalised as rules&#xA;Provide immediate feedback to users performing data correction tasks&#xA;Data quality metrics drive feedback to correct source systems as BAU activity&#xA;Define data quality metrics and a standard for all our data assets and ensure they meet the standard required by the business.&#xA;Dataset measured to be at acceptable standard for a particular purpose is issued a kite-mark&#xA;Improve resource planning capability. &#xA;Improve resource tracking capability&#xA;Remove &quot;siloed thinking&quot;. Data should be an asset available to approved and authorisedusers&#xA;Provide immediate feedback to users performing data correction tasks&#xA;&#xA;Strategies&#xA;Data sets are analysed and assessed in order to measure and quantify their conformance to data quality expectations, i.e. the extent to which the data is ‘fit for purpose’.&#xA;For any forms designed to capture information of which accurate data quality is business-critical (e.g. geographical XY coordinates), then those entry mechanisms are implemented with built-in, robust data validation, such that values entered conform to a certain format, domain, or range and that those validation mechanisms cannot be circumvented.&#xA;Where data incompleteness, inaccuracy, or outdatedness can be traced to poor business logic or lack of data integration, then systematic failings are addressed, managed, and resolved, so as to address the root causes of data quality at the point of origin, rather than cleanse the data at the point of consumption.&#xA;Where data incompleteness, inaccuracy, or outdatedness can be traced to poor data capture, then behavioural failings are addressed, managed, and resolved, so as to address the root causes of data quality at the point of origin, rather than cleanse the data at the point of consumption.&#xA;Data quality expectations are formalised as rules concerning the accuracy, completeness, consistency, integrity, timeliness, uniqueness, and validity of data, to enable the systematic identification of data quality exceptions.&#xA;Our data quality framework and associated data quality metrics enable our partners and suppliers to deliver information systems to the required standard.&#xA;All forms entry used for business-critical data capture should have format, range and domain validation mechanisms that cannot be circumvented.&#xA;Consolidation and rationalisation is applied wherever accurate data capture is compromised by an excessive number of separate forms, systems, or processes&#xA;The act of data submission, and in particular submission of a data correction, generates some type of immediate confirmation message to the user making the submission, to the effect that their submission is acknowledged, recognised, and accepted, thereby promoting the value of their contribution.&#xA;Data quality metrics drive a feedback loop, with data governance procedures and responsibilities in place to manage data quality issue resolution, and improve data quality over time, as a Business As Usual activity.&#xA;Data sets are scored according to well established data quality rules and standards, and given a kite-mark to show they have achieved the standard the business requires to do its job.&#xA;Digital data captured about the work we do and the quality of information we process can be used to measure and reward performance consistently, transparently and non-subjectively.&#xA;Data owners are available to receive notification of, and act to resolve data quality issues within their remit.&#xA;Data governance policies, standards, and procedures are in place to ensure that data retention is planned, and that decisions are made early on in the data development and design process, so that for every project an agreement is reached on how to manage the storage of relevant data over its useful lifetime.&#xA;Data captured about the status of the work we do (what we are doing, when it is being done, where it is being done, who is doing it, vehicles and tools that are being used, etc.) can be shared and tracked by those who (internally and externally) are  interested in what is going on.&#xA;The act of data submission, and in particular submission of a data correction, generates some type of immediate confirmation message to the user making the submission, to the effect that their submission is acknowledged, recognised, and accepted, thereby promoting the value of their contribution.&#xA;Based on a formalised measurement of data quality, wherever data incompleteness, inaccuracy, or outdatedness can be traced to poor quality data capture, then a feedback loop is in place to address, manage, and resolve behavioural failings.&#xA;Consolidation and rationalisation is applied wherever accurate data capture is compromised by an excessive number of separate forms, systems, or processes&#xA;Data collected at granularity to reflect physical process being modelled&#xA;Actions&#xA; Define data quality metrics and a standard for all our data assets and ensure they meet the standard required by the business.&#xA;Ensure that business subject matter experts and information leaders are accountable for achieving the standards set.&#xA;Reduce the volume of Access databases from 500,000 to less than1,000.&#xA;Outcomes&#xA;Performance indicators&#xA;Management planners&#xA;Review program&#xA;Definitions&#xA;&#xA;This policy will be evaluated in detail through the ongoing work of the Enterprise Data Architecture project. The high level concepts and definitions are defined below.&#xA;We measure and display data quality scores for all business-critical datasets to.&#xA;Each score will be based on six data quality dimensions:&#xA;Accuracy &lt;Definition>&#xA;Completeness&lt;Definition>&#xA;Consistency &lt;Definition>&#xA;Timeliness &lt;Definition>&#xA;Uniqueness &lt;Definition>&#xA;Data assets have a defined quality benchmark, which will be the threshold against which this data is measured as fit-for-purpose. These will in effect be business rules, considering the purpose the data will be used for to determine an acceptable quality threshold.&#xA;By scoring data quality, we will be able to:&#xA;Identify unreliable or duplicate data/data sets and take action to improve them&#xA;Provide assurance when using data to drive decision-making &#xA;Build trust in good data &#xA;Measure progress and trends in the quality of our data assets&#xA;It is the responsibility of the data asset owner &lt;LINK TO DATA OWNERSHIP POLICY> to manage the definition and update of the business rules determining the asset’s data quality threshold. Additionally, Data Owners are responsible for maintaining high data quality and taking appropriate corrective measures when necessary.&#xA;We commit to maintaining high quality data, using data quality scores to ensure the data we are using is fit-for-purpose and constantly improve the value of our data assets."/>
